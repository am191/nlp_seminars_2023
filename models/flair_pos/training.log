2022-12-14 21:18:29,899 ----------------------------------------------------------------------------------------------------
2022-12-14 21:18:29,900 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.1, inplace=False)
        (encoder): Embedding(11854, 100)
        (rnn): LSTM(100, 2048)
        (decoder): Linear(in_features=2048, out_features=11854, bias=True)
      )
    )
    (list_embedding_1): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.1, inplace=False)
        (encoder): Embedding(11854, 100)
        (rnn): LSTM(100, 2048)
        (decoder): Linear(in_features=2048, out_features=11854, bias=True)
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=4096, out_features=4096, bias=True)
  (rnn): LSTM(4096, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=75, bias=True)
  (loss_function): ViterbiLoss()
  (crf): CRF()
)"
2022-12-14 21:18:29,901 ----------------------------------------------------------------------------------------------------
2022-12-14 21:18:29,901 Corpus: "Corpus: 11393 train + 1266 dev + 2333 test sentences"
2022-12-14 21:18:29,902 ----------------------------------------------------------------------------------------------------
2022-12-14 21:18:29,902 Parameters:
2022-12-14 21:18:29,902  - learning_rate: "0.200000"
2022-12-14 21:18:29,903  - mini_batch_size: "8"
2022-12-14 21:18:29,903  - patience: "3"
2022-12-14 21:18:29,903  - anneal_factor: "0.5"
2022-12-14 21:18:29,904  - max_epochs: "250"
2022-12-14 21:18:29,904  - shuffle: "True"
2022-12-14 21:18:29,904  - train_with_dev: "False"
2022-12-14 21:18:29,905  - batch_growth_annealing: "False"
2022-12-14 21:18:29,905 ----------------------------------------------------------------------------------------------------
2022-12-14 21:18:29,905 Model training base path: "models211\pos\-2resume"
2022-12-14 21:18:29,906 ----------------------------------------------------------------------------------------------------
2022-12-14 21:18:29,906 Device: cuda:0
2022-12-14 21:18:29,906 ----------------------------------------------------------------------------------------------------
2022-12-14 21:18:29,907 Embeddings storage mode: none
2022-12-14 21:18:29,907 ----------------------------------------------------------------------------------------------------
2022-12-14 21:18:29,908 ----------------------------------------------------------------------------------------------------
2022-12-14 21:18:29,908 learning rate too small - quitting training!
2022-12-14 21:18:29,908 ----------------------------------------------------------------------------------------------------
2022-12-14 21:18:30,703 ----------------------------------------------------------------------------------------------------
2022-12-14 21:18:30,704 Testing using last state of model ...
2022-12-14 21:19:32,708 Evaluating as a multi-label problem: False
2022-12-14 21:19:32,876 0.9335	0.9335	0.9335	0.9335
2022-12-14 21:19:32,877 
Results:
- F-score (micro) 0.9335
- F-score (macro) 0.549
- Accuracy 0.9335

By class:
              precision    recall  f1-score   support

        NOUN     0.9253    0.9485    0.9367      9123
       PUNCT     0.9998    1.0000    0.9999      6595
        VERB     0.9217    0.9159    0.9188      5578
         ADV     0.8802    0.8983    0.8891      2134
        PRON     0.9536    0.9626    0.9581      2005
         ADJ     0.8890    0.7867    0.8347      1730
         ADP     0.9887    0.9960    0.9923      1492
       PROPN     0.8715    0.9294    0.8995      1445
       CCONJ     0.9457    0.9359    0.9408      1451
         AUX     0.8693    0.9259    0.8967       891
         DET     0.9092    0.9021    0.9056       899
        PART     0.9242    0.7786    0.8452       908
       SCONJ     0.8838    0.9006    0.8921       785
         NUM     0.9335    0.9512    0.9423       369
           X     0.8424    0.7354    0.7853       189
         SYM     0.7593    0.7885    0.7736        52
        INTJ     0.8636    0.2714    0.4130        70
         200     0.0000    0.0000    0.0000         2
         147     0.0000    0.0000    0.0000         1
          80     0.0000    0.0000    0.0000         1
          30     0.0000    0.0000    0.0000         1
          50     0.0000    0.0000    0.0000         1
          34     0.0000    0.0000    0.0000         1
         140     0.0000    0.0000    0.0000         1
         618     0.0000    0.0000    0.0000         1
         287     0.0000    0.0000    0.0000         1
          35     0.0000    0.0000    0.0000         1

    accuracy                         0.9335     35727
   macro avg     0.5689    0.5417    0.5490     35727
weighted avg     0.9332    0.9335    0.9326     35727

2022-12-14 21:19:32,878 ----------------------------------------------------------------------------------------------------
